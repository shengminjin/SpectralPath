function [trainedClassifier, validationAccuracy] = trainKNN(trainingData)
% [trainedClassifier, validationAccuracy] = trainClassifier(trainingData)
% returns a trained classifier and its accuracy. This code recreates the
% classification model trained in Classification Learner app. Use the
% generated code to automate training the same model with new data, or to
% learn how to programmatically train models.
%
%  Input:
%      trainingData: a table containing the same predictor and response
%       columns as imported into the app.
%
%  Output:
%      trainedClassifier: a struct containing the trained classifier. The
%       struct contains various fields with information about the trained
%       classifier.
%
%      trainedClassifier.predictFcn: a function to make predictions on new
%       data.
%
%      validationAccuracy: a double containing the accuracy in percent. In
%       the app, the History list displays this overall accuracy score for
%       each model.
%
% Use the code to train the model with new data. To retrain your
% classifier, call the function from the command line with your original
% data or new data as the input argument trainingData.
%
% For example, to retrain a classifier trained with the original data set
% T, enter:
%   [trainedClassifier, validationAccuracy] = trainClassifier(T)
%
% To make predictions with the returned 'trainedClassifier' on new data T2,
% use
%   yfit = trainedClassifier.predictFcn(T2)
%
% T2 must be a table containing at least the same predictor columns as used
% during training. For details, enter:
%   trainedClassifier.HowToPredict

% Auto-generated by MATLAB on 04-Jun-2021 11:05:43


% Extract predictors and response
% This code processes the data into the right shape for training the
% model.
inputTable = trainingData;
predictorNames = {'features1', 'features2', 'features3', 'features4', 'features5', 'features6', 'features7', 'features8', 'features9', 'features10', 'features11', 'features12', 'features13', 'features14', 'features15', 'features16', 'features17', 'features18', 'features19', 'features20', 'features21', 'features22', 'features23', 'features24', 'features25', 'features26', 'features27', 'features28', 'features29', 'features30', 'features31', 'features32', 'features33', 'features34', 'features35', 'features36', 'features37', 'features38', 'features39', 'features40', 'features41', 'features42', 'features43', 'features44', 'features45', 'features46', 'features47', 'features48', 'features49', 'features50', 'features51', 'features52', 'features53', 'features54', 'features55', 'features56', 'features57', 'features58', 'features59', 'features60', 'features61', 'features62', 'features63', 'features64', 'features65', 'features66', 'features67', 'features68', 'features69', 'features70', 'features71', 'features72', 'features73', 'features74', 'features75', 'features76', 'features77', 'features78', 'features79', 'features80', 'features81', 'features82', 'features83', 'features84', 'features85', 'features86', 'features87', 'features88', 'features89', 'features90', 'features91', 'features92', 'features93', 'features94', 'features95', 'features96', 'features97', 'features98', 'features99', 'features100', 'features101', 'features102', 'features103', 'features104', 'features105', 'features106', 'features107', 'features108', 'features109', 'features110', 'features111', 'features112', 'features113', 'features114', 'features115', 'features116', 'features117', 'features118', 'features119', 'features120', 'features121', 'features122', 'features123', 'features124', 'features125', 'features126', 'features127', 'features128', 'features129', 'features130', 'features131', 'features132', 'features133', 'features134', 'features135', 'features136', 'features137', 'features138', 'features139', 'features140', 'features141', 'features142', 'features143', 'features144', 'features145', 'features146', 'features147', 'features148', 'features149', 'features150', 'features151', 'features152', 'features153', 'features154', 'features155', 'features156', 'features157', 'features158', 'features159', 'features160', 'features161', 'features162', 'features163', 'features164', 'features165', 'features166', 'features167', 'features168', 'features169', 'features170', 'features171', 'features172', 'features173', 'features174', 'features175', 'features176', 'features177', 'features178', 'features179', 'features180', 'features181', 'features182', 'features183', 'features184', 'features185', 'features186', 'features187', 'features188', 'features189', 'features190', 'features191', 'features192', 'features193', 'features194', 'features195', 'features196', 'features197', 'features198', 'features199', 'features200'};
predictors = inputTable(:, predictorNames);
response = inputTable.Group;
isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Train a classifier
% This code specifies all the classifier options and trains the classifier.
classificationKNN = fitcknn(...
    predictors, ...
    response, ...
    'Distance', 'Euclidean', ...
    'Exponent', [], ...
    'NumNeighbors', 1, ...
    'DistanceWeight', 'Equal', ...
    'Standardize', true, ...
    'ClassNames', {'astroph'; 'bio-dmela'; 'bio-grid-human'; 'bio-grid-yeast'; 'brightkite'; 'condmat'; 'flixster'; 'gowalla'; 'grqc'; 'hepth'; 'human-brain'; 'hyves'; 'livejournal'; 'myspace'; 'orkut'; 'roadbel'; 'roadca'; 'roadpa'; 'roadtx'; 'youtube'});

% Create the result struct with predict function
predictorExtractionFcn = @(t) t(:, predictorNames);
knnPredictFcn = @(x) predict(classificationKNN, x);
trainedClassifier.predictFcn = @(x) knnPredictFcn(predictorExtractionFcn(x));

% Add additional fields to the result struct
trainedClassifier.RequiredVariables = {'features1', 'features2', 'features3', 'features4', 'features5', 'features6', 'features7', 'features8', 'features9', 'features10', 'features11', 'features12', 'features13', 'features14', 'features15', 'features16', 'features17', 'features18', 'features19', 'features20', 'features21', 'features22', 'features23', 'features24', 'features25', 'features26', 'features27', 'features28', 'features29', 'features30', 'features31', 'features32', 'features33', 'features34', 'features35', 'features36', 'features37', 'features38', 'features39', 'features40', 'features41', 'features42', 'features43', 'features44', 'features45', 'features46', 'features47', 'features48', 'features49', 'features50', 'features51', 'features52', 'features53', 'features54', 'features55', 'features56', 'features57', 'features58', 'features59', 'features60', 'features61', 'features62', 'features63', 'features64', 'features65', 'features66', 'features67', 'features68', 'features69', 'features70', 'features71', 'features72', 'features73', 'features74', 'features75', 'features76', 'features77', 'features78', 'features79', 'features80', 'features81', 'features82', 'features83', 'features84', 'features85', 'features86', 'features87', 'features88', 'features89', 'features90', 'features91', 'features92', 'features93', 'features94', 'features95', 'features96', 'features97', 'features98', 'features99', 'features100', 'features101', 'features102', 'features103', 'features104', 'features105', 'features106', 'features107', 'features108', 'features109', 'features110', 'features111', 'features112', 'features113', 'features114', 'features115', 'features116', 'features117', 'features118', 'features119', 'features120', 'features121', 'features122', 'features123', 'features124', 'features125', 'features126', 'features127', 'features128', 'features129', 'features130', 'features131', 'features132', 'features133', 'features134', 'features135', 'features136', 'features137', 'features138', 'features139', 'features140', 'features141', 'features142', 'features143', 'features144', 'features145', 'features146', 'features147', 'features148', 'features149', 'features150', 'features151', 'features152', 'features153', 'features154', 'features155', 'features156', 'features157', 'features158', 'features159', 'features160', 'features161', 'features162', 'features163', 'features164', 'features165', 'features166', 'features167', 'features168', 'features169', 'features170', 'features171', 'features172', 'features173', 'features174', 'features175', 'features176', 'features177', 'features178', 'features179', 'features180', 'features181', 'features182', 'features183', 'features184', 'features185', 'features186', 'features187', 'features188', 'features189', 'features190', 'features191', 'features192', 'features193', 'features194', 'features195', 'features196', 'features197', 'features198', 'features199', 'features200'};
trainedClassifier.ClassificationKNN = classificationKNN;
trainedClassifier.About = 'This struct is a trained model exported from Classification Learner R2017a.';
trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedModel''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');

% Extract predictors and response
% This code processes the data into the right shape for training the
% model.
inputTable = trainingData;
predictorNames = {'features1', 'features2', 'features3', 'features4', 'features5', 'features6', 'features7', 'features8', 'features9', 'features10', 'features11', 'features12', 'features13', 'features14', 'features15', 'features16', 'features17', 'features18', 'features19', 'features20', 'features21', 'features22', 'features23', 'features24', 'features25', 'features26', 'features27', 'features28', 'features29', 'features30', 'features31', 'features32', 'features33', 'features34', 'features35', 'features36', 'features37', 'features38', 'features39', 'features40', 'features41', 'features42', 'features43', 'features44', 'features45', 'features46', 'features47', 'features48', 'features49', 'features50', 'features51', 'features52', 'features53', 'features54', 'features55', 'features56', 'features57', 'features58', 'features59', 'features60', 'features61', 'features62', 'features63', 'features64', 'features65', 'features66', 'features67', 'features68', 'features69', 'features70', 'features71', 'features72', 'features73', 'features74', 'features75', 'features76', 'features77', 'features78', 'features79', 'features80', 'features81', 'features82', 'features83', 'features84', 'features85', 'features86', 'features87', 'features88', 'features89', 'features90', 'features91', 'features92', 'features93', 'features94', 'features95', 'features96', 'features97', 'features98', 'features99', 'features100', 'features101', 'features102', 'features103', 'features104', 'features105', 'features106', 'features107', 'features108', 'features109', 'features110', 'features111', 'features112', 'features113', 'features114', 'features115', 'features116', 'features117', 'features118', 'features119', 'features120', 'features121', 'features122', 'features123', 'features124', 'features125', 'features126', 'features127', 'features128', 'features129', 'features130', 'features131', 'features132', 'features133', 'features134', 'features135', 'features136', 'features137', 'features138', 'features139', 'features140', 'features141', 'features142', 'features143', 'features144', 'features145', 'features146', 'features147', 'features148', 'features149', 'features150', 'features151', 'features152', 'features153', 'features154', 'features155', 'features156', 'features157', 'features158', 'features159', 'features160', 'features161', 'features162', 'features163', 'features164', 'features165', 'features166', 'features167', 'features168', 'features169', 'features170', 'features171', 'features172', 'features173', 'features174', 'features175', 'features176', 'features177', 'features178', 'features179', 'features180', 'features181', 'features182', 'features183', 'features184', 'features185', 'features186', 'features187', 'features188', 'features189', 'features190', 'features191', 'features192', 'features193', 'features194', 'features195', 'features196', 'features197', 'features198', 'features199', 'features200'};
predictors = inputTable(:, predictorNames);
response = inputTable.Group;
isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Perform cross-validation
partitionedModel = crossval(trainedClassifier.ClassificationKNN, 'KFold', 10);

% Compute validation predictions
[validationPredictions, validationScores] = kfoldPredict(partitionedModel);

% Compute validation accuracy
validationAccuracy = 1 - kfoldLoss(partitionedModel, 'LossFun', 'ClassifError');
